# -*- coding: utf-8 -*-
"""DSS code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Pl_0L7C9LR1iRMK80drZOquDqQCmhX3C
"""

import pandas as pd
import streamlit as st
import pickle
import plotly.express as px
from sklearn.model_selection import train_test_split
from sklearn.impute import MissingIndicator
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import FeatureUnion
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import xgboost as xgb
from xgboost import XGBClassifier

st.title('Welcome to our decision support system!')
st.write('Here are the details:')

#dataset display
st.header('HELOC dataset')
st.write('this is an anonymized dataset of Home Equity Line of Credit (HELOC) applications made by real homeowners')
ori_data=pd.read_csv('heloc_dataset_v1.csv')
st.write(ori_data.head())
st.subheader('risk performance distribution on the HELOC dataset')
fig = px.pie(ori_data, names='RiskPerformance')

st.plotly_chart(fig)
#model develop
new_names = ['RP']
factor = 'F'
for i in range(1,24):
   name = factor + str(i)
   new_names.append(name)

missing_values = ['-7', '-8', '-9']
df = pd.read_csv('heloc_dataset_v1.csv', na_values = missing_values, names = new_names, header = 0)
df.isnull().sum()

## Check Correlation Between Features and Risk Performance"""

sns.set(style='white',context='notebook',palette='muted')

df = df.replace('Good',0)
df = df.replace('Bad',1)

plt.figure(figsize=(24,24))
sns.heatmap(df[df.columns].corr(),cmap='BrBG',annot=True,
           linewidths=.5)
plt.xticks(rotation=45)


df = df[['RP', 'F1', 'F2', 'F4', 'F5', 'F12', 'F14', 'F18', 'F23']]
df.isnull().sum()


df = df.dropna(axis=0,subset=['F1'])
df.isnull().sum()



df['F2'].fillna(df['F2'].mean(), inplace=True)
df['F18'].fillna(df['F18'].mean(), inplace=True)
df['F23'].fillna(df['F23'].mean(), inplace=True)
df.isnull().sum()



X = df.iloc[:,1:9]
Y = df.iloc[:, 0].replace({"Bad": 1, "Good": 0})

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=4321)

X_train_tr, X_train_val, Y_train_tr, Y_train_val = train_test_split(X_train, Y_train, test_size=0.25, random_state=4321)


params = {
    'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],
    'learning_rate': np.linspace(0.01, 0.5, 50),
    'subsample': np.linspace(0.5, 1, 50),
    'colsample_bytree': np.linspace(0.5, 1, 50),
    'reg_alpha': [0, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1],
    'reg_lambda': [0, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1],
    'gamma': [0, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1]
}


clf = xgb.XGBClassifier(objective='binary:logistic', eval_metric='auc')

random_search = RandomizedSearchCV(
    estimator=clf, 
    param_distributions=params, 
    n_iter=100,
    scoring='roc_auc',
    n_jobs=-1,
    cv=5,
    verbose=1, 
    random_state=1
)


random_search.fit(X_train_tr, Y_train_tr)



params = {
    'max_depth': 4,
    'learning_rate': 0.02,
    'subsample': 0.5102040816326531,
    'reg_lambda': 0.01, 
    'reg_alpha': 1,
    'gamma': 0.001,
    'colsample_bytree': 0.7244897959183674,
    'objective': 'binary:logistic',
    'eval_metric': 'auc'
}

model = XGBClassifier(**params)
model.fit(X_train_tr, Y_train_tr)

training_accuracy_xgb = accuracy_score(Y_test, model.predict(X_test))




val_accuracy_xgb = accuracy_score(Y_train_val, model.predict(X_train_val))



with open('model.p','wb') as f:
    pickle.dump(model,f)

#interaction
st.header("You can choose the application's parameters below:")


with open('model.p','rb') as f1:
    loaded_model = pickle.load(f1)
F1 = st.slider('External Risk Estimate：',1,300,5,1)
F2 = st.slider('Months Since Oldest Trade Open：',1,1000,5,1)
F4 = st.slider('Average Months in File：',1,300,5,1)
F5 = st.slider('Number Satisfactory Trades：',1,300,5,1)
F12 = st.slider('Number of Total Trades：',1,300,5,1)
F14 = st.slider('Percent Installment Trades：',1,300,5,1)
F18 = st.slider('Net Fraction Revolving Burden：',1,300,5,1)
F23 = st.slider('Percent Trades with Balance：',1,300,5,1)


data = {'F1': [F1], 'F2': [F2], 'F4': [F4], 'F5': [F5],
        'F12': [F12], 'F14': [F14], 'F18': [F18], 'F23': [F23]}
new_data = pd.DataFrame(data)

result = loaded_model.predict(new_data)

result_text = 'According to our prediction, this application is at low risk , you can consider accepting it' if result == 1 else 'According to our prediction, this application is at high risk , careful consideration is recommended'
st.header("Here is the result:")
st.subheader('%s'%(result_text))